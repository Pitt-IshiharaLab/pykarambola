{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Processing with ProcessPoolExecutor\n",
    "\n",
    "This notebook demonstrates how to use `concurrent.futures.ProcessPoolExecutor` (stdlib) to\n",
    "compute Minkowski functionals on many meshes in parallel using pykarambola."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pykarambola"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "Functions passed to `ProcessPoolExecutor` must be **top-level** (module-scope) functions,\n",
    "because Python pickles them to send across processes. Lambdas and closures won't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrays_from_triangulation(tri):\n",
    "    \"\"\"Extract vertex/face numpy arrays from a Triangulation object.\"\"\"\n",
    "    nv, nt = tri.n_vertices(), tri.n_triangles()\n",
    "    verts = np.array([tri.get_pos_of_vertex(i) for i in range(nv)], dtype=np.float64)\n",
    "    faces = np.array(\n",
    "        [[tri.ith_vertex_of_triangle(j, i) for i in range(3)] for j in range(nt)],\n",
    "        dtype=np.int64,\n",
    "    )\n",
    "    return verts, faces\n",
    "\n",
    "\n",
    "def compute_from_arrays(args):\n",
    "    \"\"\"Compute Minkowski functionals from a (verts, faces) tuple.\n",
    "\n",
    "    This is a top-level function so it can be pickled by multiprocessing.\n",
    "    \"\"\"\n",
    "    verts, faces = args\n",
    "    return pykarambola.minkowski_functionals(verts, faces)\n",
    "\n",
    "\n",
    "def compute_from_arrays_with_options(args):\n",
    "    \"\"\"Compute specific functionals with options.\n",
    "\n",
    "    args: (verts, faces, compute, center)\n",
    "    \"\"\"\n",
    "    verts, faces, compute, center = args\n",
    "    return pykarambola.minkowski_functionals(verts, faces, compute=compute, center=center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate synthetic meshes\n",
    "\n",
    "We create a batch of randomly-sized boxes to simulate a realistic workload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_box(a, b, c):\n",
    "    \"\"\"Return (verts, faces) for an axis-aligned box of size a x b x c centred at origin.\"\"\"\n",
    "    ha, hb, hc = a / 2, b / 2, c / 2\n",
    "    verts = np.array([\n",
    "        [-ha, -hb, -hc], [ ha, -hb, -hc], [ ha,  hb, -hc], [-ha,  hb, -hc],\n",
    "        [-ha, -hb,  hc], [ ha, -hb,  hc], [ ha,  hb,  hc], [-ha,  hb,  hc],\n",
    "    ], dtype=np.float64)\n",
    "    faces = np.array([\n",
    "        [0, 3, 2], [0, 2, 1],\n",
    "        [4, 5, 6], [4, 6, 7],\n",
    "        [0, 1, 5], [0, 5, 4],\n",
    "        [2, 3, 7], [2, 7, 6],\n",
    "        [0, 4, 7], [0, 7, 3],\n",
    "        [1, 2, 6], [1, 6, 5],\n",
    "    ], dtype=np.int64)\n",
    "    return verts, faces\n",
    "\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "N_MESHES = 200\n",
    "meshes = [make_box(*rng.uniform(1, 10, size=3)) for _ in range(N_MESHES)]\n",
    "\n",
    "print(f\"Generated {N_MESHES} random boxes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sequential baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.perf_counter()\n",
    "results_seq = [compute_from_arrays(m) for m in meshes]\n",
    "dt_seq = time.perf_counter() - t0\n",
    "\n",
    "print(f\"Sequential: {dt_seq:.3f}s for {N_MESHES} meshes ({dt_seq/N_MESHES*1000:.1f} ms/mesh)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parallel with ProcessPoolExecutor\n",
    "\n",
    "By default, `ProcessPoolExecutor()` uses all available CPU cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cores = os.cpu_count()\n",
    "print(f\"Available CPU cores: {n_cores}\")\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "with ProcessPoolExecutor() as pool:\n",
    "    results_par = list(pool.map(compute_from_arrays, meshes))\n",
    "dt_par = time.perf_counter() - t0\n",
    "\n",
    "print(f\"Parallel:   {dt_par:.3f}s for {N_MESHES} meshes ({dt_par/N_MESHES*1000:.1f} ms/mesh)\")\n",
    "print(f\"Speedup:    {dt_seq/dt_par:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Verify correctness\n",
    "\n",
    "Results should be identical regardless of sequential vs parallel execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (r_seq, r_par) in enumerate(zip(results_seq, results_par)):\n",
    "    for key in r_seq:\n",
    "        assert np.allclose(r_seq[key], r_par[key]), f\"Mismatch at mesh {i}, key {key}\"\n",
    "\n",
    "print(\"All results match between sequential and parallel execution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Controlling the number of workers\n",
    "\n",
    "You can limit worker count to leave cores free for other tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_workers in [1, 2, 4, os.cpu_count()]:\n",
    "    t0 = time.perf_counter()\n",
    "    with ProcessPoolExecutor(max_workers=n_workers) as pool:\n",
    "        results = list(pool.map(compute_from_arrays, meshes))\n",
    "    dt = time.perf_counter() - t0\n",
    "    print(f\"  {n_workers} workers: {dt:.3f}s  (speedup {dt_seq/dt:.2f}x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Passing options (compute subset, center)\n",
    "\n",
    "To pass extra arguments, pack them into tuples and use a wrapper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only compute volume and surface area, with centroid centering\n",
    "tasks = [(v, f, ['w000', 'w100'], 'centroid') for v, f in meshes]\n",
    "\n",
    "with ProcessPoolExecutor() as pool:\n",
    "    results_subset = list(pool.map(compute_from_arrays_with_options, tasks))\n",
    "\n",
    "print(f\"First 5 volumes:  {[r['w000'] for r in results_subset[:5]]}\")\n",
    "print(f\"First 5 areas:    {[round(r['w100'], 2) for r in results_subset[:5]]}\")\n",
    "print(f\"Keys per result:  {sorted(results_subset[0].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Processing mesh files in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_from_poly_file(filepath):\n",
    "    \"\"\"Load a .poly file and compute Minkowski functionals.\"\"\"\n",
    "    tri = pykarambola.parse_poly_file(str(filepath))\n",
    "    nv, nt = tri.n_vertices(), tri.n_triangles()\n",
    "    verts = np.array([tri.get_pos_of_vertex(i) for i in range(nv)], dtype=np.float64)\n",
    "    faces = np.array(\n",
    "        [[tri.ith_vertex_of_triangle(j, i) for i in range(3)] for j in range(nt)],\n",
    "        dtype=np.int64,\n",
    "    )\n",
    "    return {\n",
    "        'file': filepath.name,\n",
    "        'n_verts': nv,\n",
    "        'n_faces': nt,\n",
    "        'functionals': pykarambola.minkowski_functionals(verts, faces),\n",
    "    }\n",
    "\n",
    "\n",
    "poly_files = sorted(Path('../test_suite/inputs').glob('*.poly'))\n",
    "print(f\"Found {len(poly_files)} .poly files\")\n",
    "\n",
    "with ProcessPoolExecutor() as pool:\n",
    "    file_results = list(pool.map(compute_from_poly_file, poly_files))\n",
    "\n",
    "for r in file_results:\n",
    "    vol = r['functionals'].get('w000', float('nan'))\n",
    "    print(f\"  {r['file']:<50s}  V={r['n_verts']:>4d}  F={r['n_faces']:>4d}  vol={vol:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Writing results to CSV\n",
    "\n",
    "All results are collected in the main process before writing, so a plain pandas `DataFrame` is safe â€” no locking needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "rows = [\n",
    "    {\"file\": r[\"file\"], \"n_verts\": r[\"n_verts\"], \"n_faces\": r[\"n_faces\"], **r[\"functionals\"]}\n",
    "    for r in file_results\n",
    "]\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(\"results.csv\", index=False)\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Processing label images in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_from_label_image(args):\n",
    "    \"\"\"Compute Minkowski functionals from a 3D label image.\"\"\"\n",
    "    label_image, spacing = args\n",
    "    return pykarambola.minkowski_functionals_from_label_image(\n",
    "        label_image, spacing=spacing, center='centroid',\n",
    "    )\n",
    "\n",
    "\n",
    "# Generate synthetic label images: spheres of varying radii\n",
    "shape = (64, 64, 64)\n",
    "z, y, x = np.mgrid[:shape[0], :shape[1], :shape[2]]\n",
    "center = np.array(shape) / 2\n",
    "\n",
    "images = []\n",
    "radii = [10, 15, 20, 25]\n",
    "for r in radii:\n",
    "    dist = np.sqrt((x - center[2])**2 + (y - center[1])**2 + (z - center[0])**2)\n",
    "    img = np.zeros(shape, dtype=int)\n",
    "    img[dist <= r] = 1\n",
    "    images.append((img, (1.0, 1.0, 1.0)))\n",
    "\n",
    "print(f\"Processing {len(images)} label images...\")\n",
    "\n",
    "with ProcessPoolExecutor() as pool:\n",
    "    img_results = list(pool.map(compute_from_label_image, images))\n",
    "\n",
    "for r, radius in zip(img_results, radii):\n",
    "    vol = r[1]['w000']\n",
    "    expected = 4/3 * np.pi * radius**3\n",
    "    print(f\"  radius={radius:>2d}  vol={vol:>10.1f}  expected={expected:>10.1f}  err={abs(vol-expected)/expected:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips\n",
    "\n",
    "- **Process, not thread**: `ProcessPoolExecutor` spawns separate processes, bypassing the GIL.\n",
    "  `ThreadPoolExecutor` would give no speedup for CPU-bound work like this.\n",
    "- **Picklability**: All functions passed to the pool must be top-level (not lambdas or closures).\n",
    "  Inputs/outputs must be picklable (numpy arrays, dicts, floats are all fine).\n",
    "- **Chunk size**: For many small tasks, pass `chunksize=` to `pool.map()` to reduce IPC overhead.\n",
    "- **Memory**: Each worker gets a copy of the data. For very large meshes, monitor memory usage.\n",
    "- **Error handling**: Exceptions in workers are re-raised in the main process when you iterate results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
